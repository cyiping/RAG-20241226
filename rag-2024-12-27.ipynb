{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. 從 github 下載資料\n!git clone https://github.com/cyiping/RAG-20241226.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T09:22:05.625202Z","iopub.execute_input":"2024-12-27T09:22:05.625674Z","iopub.status.idle":"2024-12-27T09:22:07.707678Z","shell.execute_reply.started":"2024-12-27T09:22:05.625644Z","shell.execute_reply":"2024-12-27T09:22:07.706225Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'RAG-20241226'...\nremote: Enumerating objects: 18, done.\u001b[K\nremote: Counting objects: 100% (18/18), done.\u001b[K\nremote: Compressing objects: 100% (16/16), done.\u001b[K\nremote: Total 18 (delta 4), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (18/18), 16.95 MiB | 15.88 MiB/s, done.\nResolving deltas: 100% (4/4), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 2. 切換工作目錄\nimport os\nos.chdir('/kaggle/working/RAG-20241226')\n!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T09:22:10.940871Z","iopub.execute_input":"2024-12-27T09:22:10.941232Z","iopub.status.idle":"2024-12-27T09:22:11.072732Z","shell.execute_reply.started":"2024-12-27T09:22:10.941207Z","shell.execute_reply":"2024-12-27T09:22:11.071021Z"}},"outputs":[{"name":"stdout","text":"rag-2024-12-26.ipynb  柯文哲起訴書-2024-12-26.pdf\nREADME.md\t      高虹安-李正皓-113年度訴字第27號.pdf\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# 3. 安裝套件\n!pip install PyPDF2\n!pip install sentence-transformers faiss-cpu transformers\nprint(\"---- ok !\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T09:22:14.467636Z","iopub.execute_input":"2024-12-27T09:22:14.468011Z","iopub.status.idle":"2024-12-27T09:22:27.356492Z","shell.execute_reply.started":"2024-12-27T09:22:14.467983Z","shell.execute_reply":"2024-12-27T09:22:27.354996Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Collecting PyPDF2\n  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\nDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: PyPDF2\nSuccessfully installed PyPDF2-3.0.1\nCollecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu, sentence-transformers\nSuccessfully installed faiss-cpu-1.9.0.post1 sentence-transformers-3.3.1\n---- ok !\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### RAG 程式的設計理念（適用於 Kaggle）\n#### 1. 資料預處理：\n- 使用 PyPDF2 或 pdfplumber 從上傳的 PDF 檔案中提取文字。\n- 清理並將提取的文字進行分詞，以便後續處理。\n\n#### 2. 索引建立：\n- 使用 FAISS 等向量檢索工具，透過 SentenceTransformers 模型生成文本嵌入。\n- 將這些嵌入儲存以進行高效的相似度搜尋。\n\n#### 3. 文本檢索：\n- 使用 FAISS 根據使用者的查詢檢索相關的文本區塊。\n\n#### 4. 答案生成：\n- 使用預訓練的語言模型（例如 OpenAI GPT 或 Hugging Face 模型）根據檢索到的區塊生成答案。\n\n#### 5. 流程整合：\n- 將檢索與生成步驟整合，完成一個流暢的 RAG 管線。","metadata":{}},{"cell_type":"markdown","source":"# 採用facebook/mbart-large-50","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport PyPDF2\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# 從 PDF 提取文字\ndef extract_text_from_pdf(pdf_path):\n    with open(pdf_path, 'rb') as f:\n        reader = PyPDF2.PdfReader(f)\n        text = \"\"\n        for page in reader.pages:\n            text += page.extract_text()\n    return text\n\n# 將文字分割為區塊\ndef split_text_into_chunks(text, chunk_size=500):\n    sentences = text.split('。')  # 使用適用於繁體中文的句號進行分割\n    chunks = []\n    chunk = \"\"\n    for sentence in sentences:\n        if len(chunk) + len(sentence) < chunk_size:\n            chunk += sentence + '。'\n        else:\n            chunks.append(chunk)\n            chunk = sentence + '。'\n    if chunk:\n        chunks.append(chunk)\n    return chunks\n\n# 對文字區塊進行嵌入\ndef embed_chunks(chunks, model):\n    return model.encode(chunks, convert_to_tensor=True)\n\n# 建立 FAISS 索引\ndef create_faiss_index(embeddings):\n    d = embeddings.shape[1]\n    index = faiss.IndexFlatL2(d)\n    index.add(embeddings.cpu().numpy())\n    return index\n\n# 增強的清理上下文函數\ndef clean_context(context):\n    # 移除數字、符號及多餘的空白\n    cleaned_context = re.sub(r'\\d+|[{}\\[\\],\\'\";:]', '', context)\n    cleaned_context = re.sub(r'\\s+', ' ', cleaned_context).strip()\n    return cleaned_context\n\n# 檢索相關的區塊\ndef retrieve_chunks(query, index, model, chunks, top_k=5):\n    query_embedding = model.encode([query], convert_to_tensor=True)\n    distances, indices = index.search(query_embedding.cpu().numpy(), top_k)\n    retrieved_chunks = [chunks[i] for i in indices[0] if i < len(chunks)]\n    \n    # 過濾空或無意義區塊\n    filtered_chunks = [chunk for chunk in retrieved_chunks if len(chunk.strip()) > 10]\n    if not filtered_chunks:\n        return [\"查無相關內容\"]\n    \n    return filtered_chunks\n\n# 生成答案\ndef generate_answer(query, context, tokenizer, model):\n    # 限制上下文長度避免超過模型限制\n    max_context_length = 1000  # 避免超過模型的最大 token 限制\n    context = context[:max_context_length]\n    \n    input_text = f\"context: {context} question: {query}\"\n    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True, max_length=1024)\n    outputs = model.generate(inputs, max_length=200, num_beams=5, early_stopping=True)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# RAG 主流程\ndef rag_pipeline(pdf_path, query):\n    # 步驟 1：提取並預處理\n    text = extract_text_from_pdf(pdf_path)\n    chunks = split_text_into_chunks(text)\n\n    # 步驟 2：嵌入與索引\n    embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n    embeddings = embed_chunks(chunks, embed_model)\n    index = create_faiss_index(embeddings)\n\n    # 步驟 3：檢索相關區塊\n    relevant_chunks = retrieve_chunks(query, index, embed_model, chunks)\n\n    # 清理上下文\n    context = clean_context(\" \".join(relevant_chunks))\n\n    # 步驟 4：生成答案\n    tokenizer = AutoTokenizer.from_pretrained('facebook/mbart-large-50')\n    gen_model = AutoModelForSeq2SeqLM.from_pretrained('facebook/mbart-large-50')\n    tokenizer.src_lang = \"zh_CN\"  # 設置源語言為中文\n    answer = generate_answer(query, context, tokenizer, gen_model)\n\n    return answer\n\n# 使用範例\npdf_path = '高虹安-李正皓-113年度訴字第27號.pdf'  # 替換為您的 PDF 檔案路徑\nquery = \"判決的關鍵理由是什麼？\"\nanswer = rag_pipeline(pdf_path, query)\nprint(answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T09:22:31.861763Z","iopub.execute_input":"2024-12-27T09:22:31.862110Z","iopub.status.idle":"2024-12-27T09:24:31.198163Z","shell.execute_reply.started":"2024-12-27T09:22:31.862085Z","shell.execute_reply":"2024-12-27T09:24:31.196850Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abd8810a65a24932b874a47588186881"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66c6ee86675e4847b106e6974e811350"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"276dfd8803f144789220babce9a35b20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"180abdfd229849619678676ee5f37bdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9e2d8919e42452f87e6f06b4054deca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78140a431aa14bfba25347cee64ec80c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"825768c7bd2941d8a17fb8931a5aafe1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3879bd80cb7240339e6977e127476fea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e2257678d7d456991e84f7d96e53eda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db683b2013fc4d7c8c09e8d3bd4cfdfb"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0894f78894a4d50817d14eda6897533"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"349f0f63a3b64621811515c29927d06d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"164d7015de9949b584a1f09ba5649e67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/531 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00c947fe31d645a18bbcb4e4dba5e8af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45094dc349784228bb5773a738c6644f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86d050126a554681ac0733c89bb1cdc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d907c45ccd3a402a90a3778cc13945b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee06b56d1aeb482bb6ca20a00958416c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11fc47c6aba84195ab3cef570a5eaf76"}},"metadata":{}},{"name":"stdout","text":"context: (續上頁) 第二十八頁、李正皓:第一個發言人你先回一 下記者的訊息,發言人的爭議我 們可能晚點聊,發言人現在連專 門挖弊案的大記者的訊息從去年 未讀到現在,發言人先把記者的 問題回一回再說,還嫌人家問題 太多。 又被證 該資料,其內記載A類即協議合建都更案,審議時間最短 約.年(見本院卷二第頁),亦較系爭昇益案進行所 花費之約年個月時間為長。 、次查,被證於年月日之媒體報導,載稱於年 月日,台北地檢署偵辦高虹安涉詐領助理費案,傳喚高虹 安、男友李忠庭等(見本院\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"query = \"李正皓的判決是？\"\nanswer = rag_pipeline(pdf_path, query)\nprint(answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T09:25:35.273063Z","iopub.execute_input":"2024-12-27T09:25:35.274096Z","iopub.status.idle":"2024-12-27T09:26:53.428136Z","shell.execute_reply.started":"2024-12-27T09:25:35.274052Z","shell.execute_reply":"2024-12-27T09:26:53.426961Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"257882512fa349fcb4984a66b112550a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e97473bf71be4e6c9906ffc6a5864f0a"}},"metadata":{}},{"name":"stdout","text":"context: (續上頁) 第二十八頁、李正皓:第一個發言人你先回一 下記者的訊息,發言人的爭議我 們可能晚點聊,發言人現在連專 門挖弊案的大記者的訊息從去年 未讀到現在,發言人先把記者的 問題回一回再說,還嫌人家問題 太多。 、次查,被證於年月日之媒體報導,載稱於年 月日,台北地檢署偵辦高虹安涉詐領助理費案,傳喚高虹 安、男友李忠庭等(見本院卷一第頁),已報導提及李 忠庭係原告高虹安之男友,而此亦為原告在本件所未爭執。 、李正皓:奇怪?好像高虹安市府 講大家都是笨蛋,解凍就是一種\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# 採用 uer/t5-base-chinese-cluecorpussmall","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport PyPDF2\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# 從 PDF 提取文字\ndef extract_text_from_pdf(pdf_path):\n    with open(pdf_path, 'rb') as f:\n        reader = PyPDF2.PdfReader(f)\n        text = \"\"\n        for page in reader.pages:\n            page_text = page.extract_text()\n            # 移除換行符並合併行\n            page_text = re.sub(r'\\n', '', page_text)\n            text += page_text\n    return text\n\n# 將文字分割為區塊\ndef split_text_into_chunks(text, chunk_size=500):\n    sentences = text.split('。')  # 使用適用於繁體中文的句號進行分割\n    chunks = []\n    chunk = \"\"\n    for sentence in sentences:\n        if len(chunk) + len(sentence) < chunk_size:\n            chunk += sentence + '。'\n        else:\n            chunks.append(chunk)\n            chunk = sentence + '。'\n    if chunk:\n        chunks.append(chunk)\n    return chunks\n\n# 對文字區塊進行嵌入\ndef embed_chunks(chunks, model):\n    return model.encode(chunks, convert_to_tensor=True)\n\n# 建立 FAISS 索引\ndef create_faiss_index(embeddings):\n    d = embeddings.shape[1]\n    index = faiss.IndexFlatL2(d)\n    index.add(embeddings.cpu().numpy())\n    return index\n\n# 增強的清理上下文函數\ndef clean_context(context):\n    # 僅保留中文文字與基本標點符號（如句號、逗號）\n    cleaned_context = re.sub(r'[^一-龥。，！？]', '', context)\n    cleaned_context = re.sub(r'\\s+', ' ', cleaned_context).strip()\n    return cleaned_context\n\n# 檢索相關的區塊\ndef retrieve_chunks(query, index, model, chunks, top_k=5):\n    query_embedding = model.encode([query], convert_to_tensor=True)\n    distances, indices = index.search(query_embedding.cpu().numpy(), top_k)\n    retrieved_chunks = [chunks[i] for i in indices[0] if i < len(chunks)]\n    \n    # 增加關鍵詞過濾，確保相關性\n    filtered_chunks = [chunk for chunk in retrieved_chunks if query in chunk]\n    if not filtered_chunks:\n        return [\"查無相關內容\"]\n    \n    return filtered_chunks\n\n# 生成答案\ndef generate_answer(query, context, tokenizer, model):\n    # 限制上下文長度避免超過模型限制\n    max_context_length = 1000\n    context = context[:max_context_length]\n    \n    input_text = f\"問題：{query} 上下文：{context}\"\n    print(\"生成模型輸入:\", input_text)  # Debug 輸入文本\n    \n    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True, max_length=1024)\n    outputs = model.generate(inputs, max_length=200, num_beams=5, early_stopping=True)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# RAG 主流程\ndef rag_pipeline(pdf_path, query):\n    # 步驟 1：提取並預處理\n    text = extract_text_from_pdf(pdf_path)\n    chunks = split_text_into_chunks(text)\n\n    # 步驟 2：嵌入與索引\n    embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n    embeddings = embed_chunks(chunks, embed_model)\n    index = create_faiss_index(embeddings)\n\n    # 步驟 3：檢索相關區塊\n    relevant_chunks = retrieve_chunks(query, index, embed_model, chunks)\n\n    # 清理上下文\n    context = clean_context(\" \".join(relevant_chunks))\n    print(\"清理後的上下文:\", context)  # Debug 清理後的上下文\n\n    # 步驟 4：生成答案\n    tokenizer = AutoTokenizer.from_pretrained('uer/t5-base-chinese-cluecorpussmall')\n    gen_model = AutoModelForSeq2SeqLM.from_pretrained('uer/t5-base-chinese-cluecorpussmall')\n    answer = generate_answer(query, context, tokenizer, gen_model)\n\n    return answer\n\n# 使用範例\npdf_path = '高虹安-李正皓-113年度訴字第27號.pdf'  # 替換為您的 PDF 檔案路徑\nquery = \"判決的關鍵理由是什麼？\"\nanswer = rag_pipeline(pdf_path, query)\nprint(\"最終答案:\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T09:36:39.705830Z","iopub.execute_input":"2024-12-27T09:36:39.706295Z","iopub.status.idle":"2024-12-27T09:37:03.304783Z","shell.execute_reply.started":"2024-12-27T09:36:39.706247Z","shell.execute_reply":"2024-12-27T09:37:03.303589Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56e00724a0bc4923b43156894229c63f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0743ab87be204f208a8a8348f3c74e89"}},"metadata":{}},{"name":"stdout","text":"清理後的上下文: 查無相關內容\n生成模型輸入: 問題：判決的關鍵理由是什麼？ 上下文：查無相關內容\n最終答案: extra0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"query = \"總結這份文件\"\nanswer = rag_pipeline(pdf_path, query)\nprint(\"最終答案:\", answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T09:39:41.516899Z","iopub.execute_input":"2024-12-27T09:39:41.517394Z","iopub.status.idle":"2024-12-27T09:40:04.665829Z","shell.execute_reply.started":"2024-12-27T09:39:41.517358Z","shell.execute_reply":"2024-12-27T09:40:04.664676Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"781c5cda76884e4497ecf6ec9c316af6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39827b22817945c9b9b8a1ad6f8d1142"}},"metadata":{}},{"name":"stdout","text":"清理後的上下文: 查無相關內容\n生成模型輸入: 問題：總結這份文件 上下文：查無相關內容\n最終答案: extra0 extra2\n","output_type":"stream"}],"execution_count":12}]}